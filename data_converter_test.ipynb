{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b87159",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946213cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Sequence, Optional, Union\n",
    "import shutil\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "from shapely.geometry import Polygon\n",
    "from shapely import contains_xy\n",
    "\n",
    "\n",
    "def read_ais_df(\n",
    "    csv_path: Union[Path, str],\n",
    "    bbox: Sequence[float],\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read AIS CSV into a DataFrame using DuckDB and apply basic cleaning.\n",
    "\n",
    "    Operations:\n",
    "    - Spatial bbox filter in the SQL\n",
    "    - Rename & parse Timestamp\n",
    "    - Drop rows with invalid timestamps\n",
    "    - Check required columns exist\n",
    "    - Ensure MMSI is a string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : Path | str\n",
    "        Path to the AIS CSV file.\n",
    "    bbox : Sequence[float]\n",
    "        Bounding box as [lat_max, lon_min, lat_min, lon_max].\n",
    "    verbose : bool, optional\n",
    "        If True, print basic info about the loaded data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        AIS data within the given bounding box, with basic cleaning applied.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> bbox = [57.58, 10.5, 57.12, 11.92]\n",
    "    >>> df_raw = read_ais_df(\"ais-data/aisdk-2025-11-05.csv\", bbox, verbose=True)\n",
    "    \"\"\"\n",
    "    lat_max, lon_min, lat_min, lon_max = bbox\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_csv_auto('{csv_path}', AUTO_DETECT=TRUE)\n",
    "    WHERE Latitude <= {lat_max}\n",
    "      AND Latitude >= {lat_min}\n",
    "      AND Longitude >= {lon_min}\n",
    "      AND Longitude <= {lon_max}\n",
    "    ;\n",
    "    \"\"\"\n",
    "\n",
    "    df = duckdb.query(query).to_df()\n",
    "\n",
    "    # Rename Timestamp column and parse to datetime\n",
    "    df = df.rename(columns={\"# Timestamp\": \"Timestamp\"})\n",
    "    df[\"Timestamp\"] = pd.to_datetime(\n",
    "        df[\"Timestamp\"], format=\"%d/%m/%Y %H:%M:%S\", errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Drop rows where timestamp parsing failed\n",
    "    df = df.dropna(subset=[\"Timestamp\"])\n",
    "\n",
    "    # Basic column checks\n",
    "    required_columns = [\"Latitude\", \"Longitude\", \"Timestamp\", \"MMSI\", \"SOG\"]\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\" Required columns missing: {missing}\")\n",
    "\n",
    "    # Ensure MMSI is string for later processing\n",
    "    df[\"MMSI\"] = df[\"MMSI\"].astype(str)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\" Read AIS data: {len(df):,} rows within bbox, \"\n",
    "            f\" {df['MMSI'].nunique():,} unique vessels\"\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_ais_df(\n",
    "    df: pd.DataFrame,\n",
    "    polygon_coords: Sequence[tuple[float, float]],\n",
    "    allowed_mobile_types: Optional[Sequence[str]] = (\"Class A\", \"Class B\"),\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply AIS filtering steps to a DataFrame.\n",
    "\n",
    "    Steps:\n",
    "    1) Filter by \"Type of mobile\" (default: keep only \"Class A\" and \"Class B\")\n",
    "    2) MMSI sanity checks (length == 9 and MID in [200, 775])\n",
    "    3) Drop duplicates on (Timestamp, MMSI)\n",
    "    4) Polygon filtering using Shapely (lon, lat)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input AIS DataFrame with at least the columns:\n",
    "        [\"Latitude\", \"Longitude\", \"Timestamp\", \"MMSI\"].\n",
    "    polygon_coords : Sequence[tuple[float, float]]\n",
    "        Polygon vertices as (lon, lat) pairs.\n",
    "    allowed_mobile_types : Sequence[str] or None, optional\n",
    "        Types of mobile to keep (e.g., [\"Class A\", \"Class B\"]).\n",
    "        If None, the \"Type of mobile\" filter is skipped (if the column exists).\n",
    "    verbose : bool, optional\n",
    "        If True, print detailed filtering information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Filtered AIS DataFrame.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> polygon_coords = [\n",
    "    ...     (10.5162, 57.3500),\n",
    "    ...     (10.9314, 57.5120),\n",
    "    ...     (11.5128, 57.5785),\n",
    "    ...     (11.9132, 57.5230),\n",
    "    ...     (11.9189, 57.4078),\n",
    "    ...     (11.2133, 57.1389),\n",
    "    ...     (11.0067, 57.1352),\n",
    "    ...     (10.5400, 57.1880),\n",
    "    ...     (10.5162, 57.3500),\n",
    "    ... ]\n",
    "    >>> df_filt = filter_ais_df(df_raw, polygon_coords, verbose=True)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\" [filter_ais_df] Before filtering: {len(df):,} rows, \"\n",
    "            f\" [filter_ais_df] {df['MMSI'].nunique():,} unique vessels\"\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) Filter by Type of mobile (keep only selected types)\n",
    "    # ------------------------------------------------------------------\n",
    "    if \"Type of mobile\" in df.columns:\n",
    "        if allowed_mobile_types is not None:\n",
    "            before_rows = len(df)\n",
    "            df = df[df[\"Type of mobile\"].isin(allowed_mobile_types)].copy()\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\" [filter_ais_df] Type of mobile filtering complete: {len(df):,} rows \"\n",
    "                    f\" [filter_ais_df] (removed {before_rows - len(df):,} rows) \"\n",
    "                    f\" [filter_ais_df] using types: {list(allowed_mobile_types)}\"\n",
    "                )\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \" [filter_ais_df] allowed_mobile_types is None, skipping \"\n",
    "                    \" [filter_ais_df] 'Type of mobile' filtering step.\"\n",
    "                )\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\" [filter_ais_df] Warning: 'Type of mobile' column not found, skipping that filter.\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) MMSI sanity filters (format + MID)\n",
    "    # ------------------------------------------------------------------\n",
    "    # Always start from a clean string\n",
    "    mmsi_str = df[\"MMSI\"].astype(str).str.strip()\n",
    "\n",
    "    # Valid MMSI must have 9 digits\n",
    "    mask_len = mmsi_str.str.len() == 9\n",
    "\n",
    "    # First 3 digits = MID, must be numeric and in [200, 775]\n",
    "    mid = mmsi_str.str[:3]\n",
    "    mask_mid = mid.str.isnumeric() & mid.astype(int).between(200, 775)\n",
    "\n",
    "    # Combine masks\n",
    "    valid_mmsi_mask = mask_len & mask_mid\n",
    "\n",
    "    # Apply once, with aligned index\n",
    "    df = df[valid_mmsi_mask].copy()\n",
    "\n",
    "    # Update MMSI column with cleaned values\n",
    "    df[\"MMSI\"] = mmsi_str[valid_mmsi_mask]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\" [filter_ais_df] MMSI filtering complete: {len(df):,} rows, \"\n",
    "            f\" [filter_ais_df] {df['MMSI'].nunique():,} unique vessels\"\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Drop duplicates on (Timestamp, MMSI)\n",
    "    # ------------------------------------------------------------------\n",
    "    df = df.drop_duplicates([\"Timestamp\", \"MMSI\"], keep=\"first\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\" [filter_ais_df] Duplicate removal complete: {len(df):,} rows, \"\n",
    "            f\" [filter_ais_df] {df['MMSI'].nunique():,} unique vessels\"\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) Polygon filtering (FAST, vectorized)\n",
    "    #    NOTE: Shapely expects (x, y) = (lon, lat)\n",
    "    # ------------------------------------------------------------------\n",
    "    polygon = Polygon(polygon_coords)\n",
    "\n",
    "    lons = df[\"Longitude\"].to_numpy()\n",
    "    lats = df[\"Latitude\"].to_numpy()\n",
    "\n",
    "    # Vectorized containment test with Shapely 2.x\n",
    "    mask_poly = contains_xy(polygon, lons, lats)\n",
    "\n",
    "    df = df[mask_poly].copy()\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\" [filter_ais_df] Polygon filtering complete: {len(df):,} rows, \"\n",
    "            f\" [filter_ais_df] {df['MMSI'].nunique():,} unique vessels\"\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def segment_ais_tracks(\n",
    "    df: pd.DataFrame,\n",
    "    min_track_len: int = 256,\n",
    "    min_track_duration_sec: int = 60 * 60,\n",
    "    max_time_gap_sec: int = 15 * 60,\n",
    "    sog_min: Optional[float] = 0.5,\n",
    "    sog_max: Optional[float] = 25.0,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter AIS tracks and segment them by time gaps.\n",
    "\n",
    "    Assumptions\n",
    "    -----------\n",
    "    - `Timestamp` is already a datetime64 dtype.\n",
    "    - `SOG` is in **m/s**, and `sog_min` / `sog_max` (if given) are in m/s.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1) Per-MMSI track filtering:\n",
    "       - length > min_track_len\n",
    "       - duration >= min_track_duration_sec\n",
    "       - optional SOG range [sog_min, sog_max]\n",
    "    2) Sort by (MMSI, Timestamp)\n",
    "    3) Define `Segment` via time gaps > `max_time_gap_sec`\n",
    "    4) Apply the same filter at (MMSI, Segment) level\n",
    "    5) Add `Date` column: YYYY-MM-DD\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Filtered AIS DataFrame containing at least:\n",
    "        [\"MMSI\", \"Timestamp\", \"SOG\"].\n",
    "    min_track_len : int, optional\n",
    "        Minimum number of points required for track/segment.\n",
    "    min_track_duration_sec : int, optional\n",
    "        Minimum duration in seconds for track/segment.\n",
    "    max_time_gap_sec : int, optional\n",
    "        Maximum allowed time gap in seconds within a segment.\n",
    "    sog_min : float or None, optional\n",
    "        Minimum SOG (m/s) for valid track/segment. If None, no lower bound.\n",
    "    sog_max : float or None, optional\n",
    "        Maximum SOG (m/s) for valid track/segment. If None, no upper bound.\n",
    "    verbose : bool, optional\n",
    "        If True, print information about filtering and segmentation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with valid tracks, including:\n",
    "        - \"MMSI\"\n",
    "        - \"Timestamp\"\n",
    "        - \"SOG\"\n",
    "        - \"Segment\" (int)\n",
    "        - \"Date\" (str, YYYY-MM-DD)\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> df_seg = segment_ais_tracks(df_filt, verbose=True)\n",
    "    >>> df_seg[['MMSI', 'Timestamp', 'Segment']].head()\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    required_cols = [\"MMSI\", \"Timestamp\", \"SOG\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\" segment_ais_tracks: required columns missing: {missing}\")\n",
    "\n",
    "    df[\"MMSI\"] = df[\"MMSI\"].astype(str)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[\"Timestamp\"]):\n",
    "        raise TypeError(\" segment_ais_tracks: 'Timestamp' must be a datetime dtype\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\" [segment_ais_tracks] Starting with {len(df):,} rows, \"\n",
    "            f\" { df['MMSI'].nunique():,} unique vessels\"\n",
    "        )\n",
    "\n",
    "    # helper: track filter \n",
    "    def track_filter(g: pd.DataFrame) -> bool:\n",
    "        len_ok = len(g) > min_track_len\n",
    "\n",
    "        if sog_min is not None or sog_max is not None:\n",
    "            sog_max_val = g[\"SOG\"].max()\n",
    "            sog_ok = True\n",
    "            if sog_min is not None:\n",
    "                sog_ok &= sog_max_val >= sog_min\n",
    "            if sog_max is not None:\n",
    "                sog_ok &= sog_max_val <= sog_max\n",
    "        else:\n",
    "            sog_ok = True\n",
    "\n",
    "        dt = (g[\"Timestamp\"].max() - g[\"Timestamp\"].min()).total_seconds()\n",
    "        time_ok = dt >= min_track_duration_sec\n",
    "\n",
    "        return len_ok and sog_ok and time_ok\n",
    "\n",
    "    # ---------- 1) Filter per MMSI ----------\n",
    "    df = df.groupby(\"MMSI\", group_keys=False).filter(track_filter)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\" [segment_ais_tracks] After MMSI-level filter: {len(df):,} rows, \"\n",
    "            f\" {df['MMSI'].nunique():,} vessels\"\n",
    "        )\n",
    "\n",
    "    # ---------- 2) Sort ----------\n",
    "    df = df.sort_values([\"MMSI\", \"Timestamp\"])\n",
    "\n",
    "    # ---------- 3) Compute Segment IDs ----------\n",
    "    def compute_segments(ts: pd.Series) -> pd.Series:\n",
    "        gaps = ts.diff().dt.total_seconds().fillna(0)\n",
    "        return (gaps >= max_time_gap_sec).cumsum()\n",
    "\n",
    "    df[\"Segment\"] = df.groupby(\"MMSI\")[\"Timestamp\"].transform(compute_segments)\n",
    "\n",
    "    # ---------- 4) Filter per (MMSI, Segment) ----------\n",
    "    df = df.groupby([\"MMSI\", \"Segment\"], group_keys=False).filter(track_filter)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\" [segment_ais_tracks] After segment-level filter: {len(df):,} rows, \"\n",
    "            f\" {df[['MMSI','Segment']].drop_duplicates().shape[0]:,} segments\"\n",
    "        )\n",
    "\n",
    "    # ---------- 5) Add Date ----------\n",
    "    df[\"Date\"] = df[\"Timestamp\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_by_mmsi(\n",
    "    df: pd.DataFrame,\n",
    "    verbose: bool = False,\n",
    "    output_folder: Union[Path, str] = \"ais_data_parquet\",\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Write AIS data to a partitioned Parquet dataset.\n",
    "\n",
    "    The output directory is **always** \"ais_data_parquet\".\n",
    "    If it does not exist, it will be created.\n",
    "\n",
    "    IMPORTANT\n",
    "    ---------\n",
    "    This function is *overwrite-safe* for the partitions present in `df`.\n",
    "    For each unique (MMSI, Date, Segment) combination in `df`, the existing\n",
    "    partition directory is removed before writing new data. This avoids\n",
    "    accumulating multiple parquet files for the same segment when rerunning\n",
    "    the pipeline for the same date/file.\n",
    "\n",
    "    Expected columns\n",
    "    ----------------\n",
    "    df must contain:\n",
    "    - \"MMSI\"    (string-like)\n",
    "    - \"Date\"    (string, e.g. \"2025-11-05\")\n",
    "    - \"Segment\" (int)\n",
    "\n",
    "    Partition layout\n",
    "    ----------------\n",
    "    ais_data_parquet/\n",
    "        MMSI=123456789/\n",
    "            Date=2025-11-05/\n",
    "                Segment=0/part-*.parquet\n",
    "                Segment=1/part-*.parquet\n",
    "        MMSI=987654321/\n",
    "            Date=2025-11-06/\n",
    "                Segment=0/part-*.parquet\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Segmented AIS DataFrame containing \"MMSI\", \"Date\", \"Segment\".\n",
    "    verbose : bool, optional\n",
    "        If True, print the output path and some info.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the root parquet dataset folder (\"ais_data_parquet\").\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> df_seg = segment_ais_tracks(df_filt)\n",
    "    >>> out_root = save_by_mmsi(df_seg, verbose=True)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    required_cols = [\"MMSI\", \"Date\", \"Segment\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\" save_by_mmsi: required columns missing: {missing}\")\n",
    "\n",
    "    df[\"MMSI\"] = df[\"MMSI\"].astype(str)\n",
    "    \n",
    "    out_path = Path(output_folder)\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Remove existing segment folders for (MMSI, Date, Segment) in df\n",
    "    # ------------------------------------------------------------------\n",
    "    partitions = df[[\"MMSI\", \"Date\", \"Segment\"]].drop_duplicates()\n",
    "\n",
    "    for _, row in partitions.iterrows():\n",
    "        mmsi_val = row[\"MMSI\"]\n",
    "        date_val = row[\"Date\"]\n",
    "        seg_val = row[\"Segment\"]\n",
    "\n",
    "        seg_dir = (\n",
    "            out_path\n",
    "            / f\"MMSI={mmsi_val}\"\n",
    "            / f\"Date={date_val}\"\n",
    "            / f\"Segment={seg_val}\"\n",
    "        )\n",
    "\n",
    "        if seg_dir.exists():\n",
    "            if verbose:\n",
    "                print(f\" [save_by_mmsi] Removing existing partition: {seg_dir}\")\n",
    "            shutil.rmtree(seg_dir)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Write new dataset (append is fine now that partitions are cleaned)\n",
    "    # ------------------------------------------------------------------\n",
    "    table = pyarrow.Table.from_pandas(df, preserve_index=False)\n",
    "    pyarrow.parquet.write_to_dataset(\n",
    "        table,\n",
    "        root_path=str(out_path),\n",
    "        partition_cols=[\"MMSI\", \"Date\", \"Segment\"],\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\" [save_by_mmsi] Parquet dataset written/appended at: {out_path.resolve()}\")\n",
    "\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319ced9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Read AIS data: 1,001,835 rows within bbox,  225 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,001,835 rows,  [filter_ais_df] 225 unique vessels\n",
      " [filter_ais_df] Type of mobile filtering complete: 964,192 rows  [filter_ais_df] (removed 37,643 rows)  [filter_ais_df] using types: ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering complete: 964,192 rows,  [filter_ais_df] 223 unique vessels\n",
      " [filter_ais_df] Duplicate removal complete: 535,347 rows,  [filter_ais_df] 223 unique vessels\n",
      " [filter_ais_df] Polygon filtering complete: 294,200 rows,  [filter_ais_df] 157 unique vessels\n",
      " [segment_ais_tracks] Starting with 294,200 rows,  157 unique vessels\n",
      " [segment_ais_tracks] After MMSI-level filter: 186,236 rows,  124 vessels\n",
      " [segment_ais_tracks] After segment-level filter: 184,183 rows,  147 segments\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=209507000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=209535000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=211317180\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=211503430\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=211882460\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=212651000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=215114000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=218057000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=218481000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219000407\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219000407\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219000407\\Date=2025-11-05\\Segment=2\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219000407\\Date=2025-11-05\\Segment=3\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219000855\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219000855\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219001522\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219003138\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219009335\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219010207\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219010207\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219016806\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219029722\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219032670\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219554000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219592000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219592000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=219805000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220012000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220046000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220072000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220072000\\Date=2025-11-05\\Segment=2\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220127000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220127000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220127000\\Date=2025-11-05\\Segment=2\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220256000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220279000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220323000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220323000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220341000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220341000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220341000\\Date=2025-11-05\\Segment=2\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220349000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220349000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=220368000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=227000000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=230091950\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=230336000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=231522000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=231801000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=231860000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=231884000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=231897000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=232036941\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=232047453\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=235060914\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=236729000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=241143000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=241945000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=244150276\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=244635000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=245250000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=245639000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=246309000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=255802950\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=255805809\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=255805881\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=255806258\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=255915627\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=255915909\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=255915978\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=257076850\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=257086660\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=257088370\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=257127140\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=257409000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=257677000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=257872000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=258006930\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=258656000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=258809000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=259179000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=259254000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265012140\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265056000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265177000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265177000\\Date=2025-11-05\\Segment=2\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265177000\\Date=2025-11-05\\Segment=3\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265177000\\Date=2025-11-05\\Segment=4\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265410000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265410000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265410000\\Date=2025-11-05\\Segment=2\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265410000\\Date=2025-11-05\\Segment=3\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265724260\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265724260\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265778450\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265781000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=265781000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266057000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266065000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266095000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266145000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266145000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266148000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266148000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266220000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=266343000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=304927000\\Date=2025-11-05\\Segment=1\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=304927000\\Date=2025-11-05\\Segment=2\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=304957000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=305371000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=305497000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=305649000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=305774000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=311001018\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=311001499\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=314620000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=314744000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=314786000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=352001188\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=352002692\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=352003633\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=352003797\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=352005615\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=354711000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=370484000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=538007298\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=538008703\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=538010340\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=538011558\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=563271200\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=566029000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=629009048\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=629009710\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=629009770\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636015986\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636017071\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636019599\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636019890\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636020533\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636020860\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636021604\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636022993\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636023946\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636023976\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636024379\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=636025057\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Removing existing partition: ais-data-parquet\\MMSI=750399000\\Date=2025-11-05\\Segment=0\n",
      " [save_by_mmsi] Parquet dataset written/appended at: D:\\Projects\\dark-vessel-hunter\\ais-data-parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('ais-data-parquet')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call functions\n",
    "\n",
    "FOLDER_NAME = \"ais-data\"\n",
    "OUTPUT_FOLDER_NAME = \"ais-data-parquet\"\n",
    "DELETE_DOWNLOADED_CSV = False\n",
    "verbose_mode = True\n",
    "\n",
    "folder_path = Path(FOLDER_NAME)\n",
    "\n",
    "csv_path = folder_path / \"aisdk-2025-11-05.csv\"\n",
    "\n",
    "# [lat_max, lon_min, lat_min, lon_max]\n",
    "bbox = [57.58, 10.5, 57.12, 11.92]\n",
    "\n",
    "polygon_coords = [\n",
    "    (10.5162, 57.3500),  # coast top left (lon, lat)\n",
    "    (10.9314, 57.5120),  # sea top left\n",
    "    (11.5128, 57.5785),  # sea top right\n",
    "    (11.9132, 57.5230),  # top right (Swedish coast)\n",
    "    (11.9189, 57.4078),  # bottom right (Swedish coast)\n",
    "    (11.2133, 57.1389),  # sea bottom right\n",
    "    (11.0067, 57.1352),  # sea bottom left\n",
    "    (10.5400, 57.1880),  # coast bottom left\n",
    "    (10.5162, 57.3500),  # close polygon\n",
    "]\n",
    "\n",
    "# Read raw AIS data only inside bounding box\n",
    "df_raw = read_ais_df(csv_path, bbox, verbose=verbose_mode)\n",
    "\n",
    "# Filter AIS data, keeping Class A and Class B by default,\n",
    "df_filt = filter_ais_df(\n",
    "    df_raw,\n",
    "    polygon_coords,\n",
    "    allowed_mobile_types=(\"Class A\", \"Class B\"),\n",
    "    verbose=verbose_mode,\n",
    ")\n",
    "\n",
    "df_seg = segment_ais_tracks(df_filt, min_track_len=256, verbose=verbose_mode)\n",
    "save_by_mmsi(df_seg, verbose=verbose_mode, output_folder=OUTPUT_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af018c0",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aed674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional, Sequence, Union, List\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import folium\n",
    "import pyarrow  # just to keep consistent with rest of file\n",
    "\n",
    "\n",
    "def query_ais_duckdb(\n",
    "    root_path: Union[str, Path] = \"ais-data-parquet\",\n",
    "    dates: Optional[Union[str, Sequence[str]]] = None,\n",
    "    mmsi: Optional[Union[str, Sequence[str]]] = None,\n",
    "    segments: Optional[Union[int, Sequence[int]]] = None,\n",
    "    columns: Optional[Sequence[str]] = None,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fast query helper using DuckDB to read AIS data from the partitioned\n",
    "    parquet dataset generated by `ais_to_parquet`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_path : str or Path, optional\n",
    "        Root directory of the parquet dataset (default: \"ais_data_parquet\").\n",
    "    dates : str or list[str], optional\n",
    "        Date(s) to filter on (e.g. \"2025-11-05\", or [\"2025-11-05\", \"2025-11-06\"]).\n",
    "        If None, no date filter is applied.\n",
    "    mmsi : str or list[str], optional\n",
    "        MMSI or list of MMSIs to filter on. If None, no MMSI filter is applied.\n",
    "    segments : int or list[int], optional\n",
    "        Segment ID(s) to filter on. If None, no segment filter is applied.\n",
    "    columns : list[str], optional\n",
    "        Subset of columns to select. If None, selects all columns (\"*\").\n",
    "    verbose : bool, optional\n",
    "        If True, prints the generated SQL query.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Result of the DuckDB query as a pandas DataFrame.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> df = query_ais_duckdb(dates=\"2025-11-05\")\n",
    "    >>> df = query_ais_duckdb(dates=\"2025-11-05\", mmsi=\"219000123\")\n",
    "    >>> df = query_ais_duckdb(\n",
    "    ...     dates=[\"2025-11-05\", \"2025-11-06\"],\n",
    "    ...     mmsi=[\"219000123\", \"219000456\"],\n",
    "    ...     columns=[\"MMSI\", \"Timestamp\", \"Latitude\", \"Longitude\"]\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    root_path = Path(root_path)\n",
    "    if not root_path.exists():\n",
    "        raise FileNotFoundError(f\"No parquet dataset found at: {root_path}\")\n",
    "\n",
    "    # Normalization helpers\n",
    "    def _to_list(x) -> Optional[List]:\n",
    "        if x is None:\n",
    "            return None\n",
    "        if isinstance(x, (str, int)):\n",
    "            return [x]\n",
    "        return list(x)\n",
    "\n",
    "    def _sql_list_str(values: Sequence[str]) -> str:\n",
    "        return \", \".join(f\"'{v}'\" for v in values)\n",
    "\n",
    "    def _sql_list_int(values: Sequence[int]) -> str:\n",
    "        return \", \".join(str(v) for v in values)\n",
    "\n",
    "    dates_list = _to_list(dates)\n",
    "    mmsi_list = _to_list(mmsi)\n",
    "    segments_list = _to_list(segments)\n",
    "\n",
    "    # Columns\n",
    "    if columns is None:\n",
    "        col_expr = \"*\"\n",
    "    else:\n",
    "        col_expr = \", \".join(columns)\n",
    "\n",
    "    parquet_glob = str(root_path / \"**\" / \"*.parquet\")\n",
    "\n",
    "    sql = f\"SELECT {col_expr} FROM read_parquet('{parquet_glob}') WHERE 1=1\"\n",
    "\n",
    "    if dates_list is not None:\n",
    "        sql += f\" AND Date IN ({_sql_list_str([str(d) for d in dates_list])})\"\n",
    "\n",
    "    if mmsi_list is not None:\n",
    "        sql += f\" AND MMSI IN ({_sql_list_str([str(m) for m in mmsi_list])})\"\n",
    "\n",
    "    if segments_list is not None:\n",
    "        sql += f\" AND Segment IN ({_sql_list_int([int(s) for s in segments_list])})\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"[query_ais_duckdb] SQL:\\n\", sql)\n",
    "\n",
    "    con = duckdb.connect(database=\":memory:\")\n",
    "    df = con.execute(sql).df()\n",
    "    con.close()\n",
    "    return df\n",
    "\n",
    "def make_ais_folium_map(\n",
    "    date: str,\n",
    "    root_path: Union[str, Path] = \"ais-data-parquet\",\n",
    "    mmsi: Optional[Union[str, Sequence[str]]] = None,\n",
    "    tiles: str = \"CartoDB positron\",\n",
    "    zoom_start: int = 8,\n",
    "    verbose: bool = False,\n",
    ") -> folium.Map:\n",
    "    \"\"\"\n",
    "    Create a Folium map of AIS tracks for a given date, optionally for only a\n",
    "    subset of vessels (MMSIs).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date : str\n",
    "        Date to visualize (e.g. \"2025-11-05\").\n",
    "    root_path : str or Path, optional\n",
    "        Root directory of the parquet dataset (default: \"ais_data_parquet\").\n",
    "    mmsi : str or list[str], optional\n",
    "        MMSI(s) to include. If None, includes all vessels on that date.\n",
    "    tiles : str, optional\n",
    "        Folium tiles style (default: \"CartoDB positron\").\n",
    "    zoom_start : int, optional\n",
    "        Initial zoom level for the map.\n",
    "    verbose : bool, optional\n",
    "        Print some info about the data being plotted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    folium.Map\n",
    "        A Folium map with vessel tracks drawn as polylines.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> m = make_ais_folium_map(\"2025-11-05\")\n",
    "    >>> m.save(\"ais_tracks_2025-11-05.html\")\n",
    "    >>>\n",
    "    >>> m = make_ais_folium_map(\"2025-11-05\", mmsi=[\"219000123\", \"219000456\"])\n",
    "    \"\"\"\n",
    "    # Query only what we need\n",
    "    df = query_ais_duckdb(\n",
    "        root_path=root_path,\n",
    "        dates=date,\n",
    "        mmsi=mmsi,\n",
    "        columns=[\"MMSI\", \"Timestamp\", \"Latitude\", \"Longitude\", \"Segment\"],\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No AIS data found for date={date} and mmsi={mmsi}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"[make_ais_folium_map] Loaded {len(df):,} rows, \"\n",
    "            f\"{df['MMSI'].nunique():,} vessels\"\n",
    "        )\n",
    "\n",
    "    # Ensure proper dtypes / ordering\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "    df = df.sort_values([\"MMSI\", \"Segment\", \"Timestamp\"])\n",
    "\n",
    "    # Map center: mean of all positions\n",
    "    center_lat = df[\"Latitude\"].mean()\n",
    "    center_lon = df[\"Longitude\"].mean()\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=zoom_start, tiles=tiles)\n",
    "\n",
    "    # Group by vessel (and optionally segment)\n",
    "    for (mmsi_val, seg_val), g in df.groupby([\"MMSI\", \"Segment\"]):\n",
    "        coords = g[[\"Latitude\", \"Longitude\"]].to_numpy().tolist()\n",
    "        if len(coords) < 2:\n",
    "            continue  # not enough points to draw a line\n",
    "\n",
    "        tooltip = f\"MMSI: {mmsi_val}, Segment: {seg_val}\"\n",
    "        folium.PolyLine(\n",
    "            locations=coords,\n",
    "            weight=2,\n",
    "            opacity=0.8,\n",
    "            tooltip=tooltip,\n",
    "        ).add_to(m)\n",
    "\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c86b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query_ais_duckdb] SQL:\n",
      " SELECT MMSI, Timestamp, Latitude, Longitude, Segment FROM read_parquet('ais-data-parquet\\**\\*.parquet') WHERE 1=1 AND Date IN ('2025-11-05')\n",
      "[make_ais_folium_map] Loaded 184,183 rows, 123 vessels\n"
     ]
    }
   ],
   "source": [
    "m = make_ais_folium_map(\"2025-11-05\", verbose=True)\n",
    "m.save(\"ais_tracks_2025-11-05.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4207b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query_ais_duckdb] SQL:\n",
      " SELECT MMSI, Timestamp, Latitude, Longitude, Segment FROM read_parquet('ais-data-parquet\\**\\*.parquet') WHERE 1=1 AND Date IN ('2025-11-02') AND MMSI IN ('219006113', '219009229')\n",
      "[make_ais_folium_map] Loaded 18,843 rows, 2 vessels\n"
     ]
    }
   ],
   "source": [
    "m = make_ais_folium_map(\n",
    "    date=\"2025-11-02\",\n",
    "    mmsi=[\"219006113\", \"219009229\"],\n",
    "    verbose=True,\n",
    ")\n",
    "m.save(\"ais_tracks_subset_2025-11-05.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b95d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 routes.\n",
      "Saved map as dkcpc_lines_bbox.html\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
